{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this notebook to write your code for problem 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scale = 1;\n",
    "tf_trans = 1;\n",
    "\n",
    "def load_data(filename, skiprows = 1):\n",
    "    return np.loadtxt(filename, skiprows=skiprows, delimiter=' ')\n",
    "\n",
    "# Load the training and test data\n",
    "data_train = load_data('data/training_data.txt', 1)\n",
    "X_train = data_train[:, 1:]\n",
    "y_train = data_train[:, 0]\n",
    "\n",
    "#y_train = keras.utils.np_utils.to_categorical(y_train_pre,num_classes=2)\n",
    "\n",
    "data_test = load_data('data/test_data.txt', 1)\n",
    "X_test = data_test[:,:]\n",
    "\n",
    "if tf_trans == 1:\n",
    "    from sklearn.feature_extraction.text import TfidfTransformer\n",
    "    tf_transformer = TfidfTransformer(use_idf=False).fit(X_train)\n",
    "    X_train = tf_transformer.transform(X_train).todense()\n",
    "    X_test = tf_transformer.transform(X_test).todense()\n",
    "\n",
    "if scale == 1:\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2C - Depth vs Width for MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in problem 2, we have conveniently provided for your use code that loads, preprocesses, and deals with the uglies of the MNIST data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Matt\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "C:\\Users\\Matt\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# load MNIST data into Keras format\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import zero_one_loss\n",
    "from itertools import product\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "\n",
    "svc = SVC(kernel='rbf',cache_size=7000,gamma=1/2500.,verbose=1)\n",
    "\n",
    "#parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
    "\n",
    "#clf = GridSearchCV(svc, parameters,verbose=1,n_jobs=-1)\n",
    "\n",
    "svc.fit(X_train,y_train)\n",
    "\n",
    "#sorted(clf.cv_results_.keys())\n",
    "\n",
    "#scores = cross_val_score(clf, X_train, y_train, cv=5,verbose=1)\n",
    "#print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "\n",
    "#clf.fit(X_train, y_train) \n",
    "\n",
    "pred_test = svc.predict(X_train)\n",
    "pred = svc.predict(X_test)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = svc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"labels_svm2.txt\",\"w\") \n",
    "file.write(\"Id,Prediction\\n\")\n",
    "for i,x in enumerate(pred):\n",
    "    file.write(\"%i,%i\\n\" % (i+1,x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 0., ..., 0., 1., 0.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.append(X_train,X_train,axis=0);\n",
    "y_train = np.append(y_train,y_train,axis=0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "n_folds = 5\n",
    "skf = StratifiedKFold(y_train, n_folds=n_folds, shuffle=True)\n",
    "\n",
    "pred = np.zeros(10000)\n",
    "\n",
    "for i, (train, test) in enumerate(skf):\n",
    "    print(\"Running Fold\", i+1, \"/\", n_folds)\n",
    "    clf = SGDClassifier(loss='squared_hinge', penalty='l1', alpha=0.0001, l1_ratio=0.4) \n",
    "\n",
    "    clf = clf.fit(X_train[train], y_train[train]) \n",
    "    #acc_pred = 1+np.sign(clf.predict(X_train[test])) \n",
    "    acc_pred = clf.predict(X_train[test])\n",
    "    acc = 1-zero_one_loss(y_train[test],acc_pred)\n",
    "    \n",
    "    print('Test accuracy:', acc)\n",
    "    \n",
    "    pred += clf.predict(X_test)\n",
    "\n",
    "y_labs = np.maximum(0.,np.sign(pred-2.5))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augment with samples from empirical distribution of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.arange(0,2000)\n",
    "train_pre = np.arange(2000,10000)\n",
    "\n",
    "X_train_t = X_train[train_pre]\n",
    "y_train_t = y_train[train_pre]\n",
    "\n",
    "doc_lengths_1 = np.count_nonzero(X_train_t[np.where(y_train_t>0)],axis=1)\n",
    "doc_lengths_1 = np.squeeze(np.asarray(doc_lengths_1))\n",
    "\n",
    "word_freqs_1 = np.sum(X_train_t[np.where(y_train_t>0)],axis=0)\n",
    "word_freqs_1 = word_freqs_1/float(np.sum(word_freqs_1))\n",
    "word_freqs_1 = np.squeeze(np.asarray(word_freqs_1))\n",
    "\n",
    "doc_lengths_0 = np.count_nonzero(X_train_t[np.where(y_train_t==0)],axis=1)\n",
    "doc_lengths_0 = np.squeeze(np.asarray(doc_lengths_0))\n",
    "\n",
    "word_freqs_0 = np.sum(X_train_t[np.where(y_train_t==0)],axis=0)\n",
    "word_freqs_0 = np.squeeze(np.asarray(word_freqs_0))\n",
    "word_freqs_0 = word_freqs_0/float(np.sum(word_freqs_0)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999472602013327"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(word_freqs_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "sum(pvals[:-1]) > 1.0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-cc29ff7de0d9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_train_extra\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mlength\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc_lengths_0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mX_train_extra\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultinomial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_freqs_0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mlength\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc_lengths_1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.RandomState.multinomial\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: sum(pvals[:-1]) > 1.0"
     ]
    }
   ],
   "source": [
    "N_extra = 80000;\n",
    "train = np.arange(2000,20000+N_extra)\n",
    "\n",
    "X_train_extra = np.zeros((N_extra,1000))\n",
    "y_train_extra = np.random.randint(0,2,N_extra)\n",
    "\n",
    "for j in np.arange(0,N_extra):\n",
    "    if y_train_extra[j] == 0:\n",
    "        length = np.random.choice(doc_lengths_0)\n",
    "        X_train_extra[j,:] = np.random.multinomial(length, word_freqs_0)\n",
    "    else:\n",
    "        length = np.random.choice(doc_lengths_1)\n",
    "        X_train_extra[j,:] = np.random.multinomial(length, word_freqs_1)\n",
    "\n",
    "X_train = np.append(X_train,X_train_extra,axis=0);\n",
    "y_train = np.append(y_train,y_train_extra,axis=0);\n",
    "#y_train = keras.utils.np_utils.to_categorical(y_train,num_classes=2)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tf_transformer = TfidfTransformer(use_idf=False).fit(X_train)\n",
    "X_train = tf_transformer.transform(X_train)\n",
    "X_test = tf_transformer.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.799\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import zero_one_loss\n",
    "from itertools import product\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "\n",
    "#pred = np.zeros(10000)\n",
    "\n",
    "clf = SGDClassifier(loss='hinge', penalty='l1', alpha=0.00001, l1_ratio=0.25,max_iter=100) \n",
    "clf = clf.fit(X_train[train], y_train[train]) \n",
    "\n",
    "acc_pred = clf.predict(X_train[test])\n",
    "acc = 1-zero_one_loss(y_train[test],acc_pred)\n",
    "\n",
    "print('Test accuracy:', acc)\n",
    "\n",
    "#pred += clf.predict(X_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "98000/98000 [==============================] - 5s 47us/step - loss: 0.3184 - acc: 0.8632\n",
      "Epoch 2/10\n",
      "98000/98000 [==============================] - 4s 40us/step - loss: 0.2948 - acc: 0.8751\n",
      "Epoch 3/10\n",
      "98000/98000 [==============================] - 4s 39us/step - loss: 0.2918 - acc: 0.8766\n",
      "Epoch 4/10\n",
      "98000/98000 [==============================] - 4s 40us/step - loss: 0.2901 - acc: 0.8773\n",
      "Epoch 5/10\n",
      "98000/98000 [==============================] - 4s 39us/step - loss: 0.2891 - acc: 0.8777\n",
      "Epoch 6/10\n",
      "98000/98000 [==============================] - 4s 39us/step - loss: 0.2885 - acc: 0.8778\n",
      "Epoch 7/10\n",
      "98000/98000 [==============================] - 4s 39us/step - loss: 0.2877 - acc: 0.8784\n",
      "Epoch 8/10\n",
      "98000/98000 [==============================] - 4s 39us/step - loss: 0.2875 - acc: 0.8782\n",
      "Epoch 9/10\n",
      "98000/98000 [==============================] - 4s 39us/step - loss: 0.2871 - acc: 0.8787\n",
      "Epoch 10/10\n",
      "98000/98000 [==============================] - 4s 39us/step - loss: 0.2869 - acc: 0.8782\n",
      "Test score: 0.33398544251918794\n",
      "Test accuracy: 0.8505\n"
     ]
    }
   ],
   "source": [
    "# Try a DNN on expanded data\n",
    "def create_model():\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(250,input_shape=(1000,)))\n",
    "    #model.add(Activation('relu'))\n",
    "    #model.add(Dropout(0.3))\n",
    "    \n",
    "    #model.add(Dense(50))\n",
    "    #model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Dense(20))\n",
    "    #model.add(Activation('relu'))\n",
    "    #model.add(Dropout(0.3))\n",
    "    \n",
    "    #---\n",
    "    #model = Sequential()\n",
    "    #model.add(Dense(100,input_shape=(1000,)))\n",
    "    #model.add(LeakyReLU(alpha=0.))\n",
    "    #model.add(Dense(1000,input_shape=(1000,)))\n",
    "    #model.add(Activation('relu'))\n",
    "    #model.add(Dense(100))\n",
    "    #model.add(Dropout(0.05))\n",
    "    \n",
    "    model.add(Dense(2))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    return model\n",
    "    #model.summary()\n",
    "    \n",
    "model = None \n",
    "model = create_model()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "fit = model.fit(X_train[train], y_train[train], batch_size=128, epochs=10, verbose=1)    \n",
    "score = model.evaluate(X_train[test], y_train[test], verbose=0)\n",
    "\n",
    "pred = model.predict(X_test)[:,1]\n",
    "\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "y_labs = np.maximum(0.,np.sign(pred-0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"labels_dnn_augmented.txt\",\"w\") \n",
    "file.write(\"Id,Prediction\\n\")\n",
    "for i,x in enumerate(y_labs):\n",
    "    file.write(\"%i,%i\\n\" % (i+1,x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<95000x1000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 2304032 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = keras.utils.np_utils.to_categorical(y_train,num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
