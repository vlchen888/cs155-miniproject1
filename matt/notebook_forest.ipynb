{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this notebook to write your code for problem 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def load_data(filename, skiprows = 1):\n",
    "    \"\"\"\n",
    "    Function loads data stored in the file filename and returns it as a numpy ndarray.\n",
    "    \n",
    "    Inputs:\n",
    "        filename: given as a string.\n",
    "        \n",
    "    Outputs:\n",
    "        Data contained in the file, returned as a numpy ndarray\n",
    "    \"\"\"\n",
    "    return np.loadtxt(filename, skiprows=skiprows, delimiter=' ')\n",
    "\n",
    "# Load the training and test data\n",
    "data_train = load_data('data/training_data.txt', 1)\n",
    "X_train = data_train[:, 1:]\n",
    "y_train = data_train[:, 0]\n",
    "#y_train = keras.utils.np_utils.to_categorical(y_train_pre,num_classes=2)\n",
    "\n",
    "data_test = load_data('data/test_data.txt', 1)\n",
    "X_test = data_test[:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "xMean = np.mean(X_train,axis=0)\n",
    "xStd = np.std(X_train,axis=0)\n",
    "X_train = (X_train - xMean)/xStd\n",
    "X_test = (X_test - xMean)/xStd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2C - Depth vs Width for MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in problem 2, we have conveniently provided for your use code that loads, preprocesses, and deals with the uglies of the MNIST data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Matt\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "C:\\Users\\Matt\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# load MNIST data into Keras format\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaBoost():\n",
    "    def __init__(self, n_clfs=100):\n",
    "        '''\n",
    "        Initialize the AdaBoost model.\n",
    "\n",
    "        Inputs:\n",
    "            n_clfs (default 100): Initializer for self.n_clfs.        \n",
    "                \n",
    "        Attributes:\n",
    "            self.n_clfs: The number of DT weak classifiers.\n",
    "            self.coefs: A list of the AdaBoost coefficients.\n",
    "            self.clfs: A list of the DT weak classifiers, initialized as empty.\n",
    "        '''\n",
    "        self.n_clfs = n_clfs\n",
    "        self.coefs = []\n",
    "        self.clfs = []\n",
    "\n",
    "    def fit(self, X, Y, n_nodes=4):\n",
    "        '''\n",
    "        Fit the AdaBoost model. Note that since we are implementing this method in a class, rather\n",
    "        than having a bunch of inputs and outputs, you will deal with the attributes of the class.\n",
    "        (see the __init__() method).\n",
    "        \n",
    "        This method should thus train self.n_clfs DT weak classifiers and store them in self.clfs,\n",
    "        with their coefficients in self.coefs.\n",
    "\n",
    "        Inputs:\n",
    "            X: A (N, D) shaped numpy array containing the data points.\n",
    "            Y: A (N, ) shaped numpy array containing the (float) labels of the data points.\n",
    "               (Even though the labels are ints, we treat them as floats.)\n",
    "            n_nodes: The max number of nodes that the DT weak classifiers are allowed to have.\n",
    "            \n",
    "        Outputs:\n",
    "            A (N, T) shaped numpy array, where T is the number of iterations / DT weak classifiers,\n",
    "            such that the t^th column contains D_{t+1} (the dataset weights at iteration t+1).\n",
    "        '''\n",
    "        \n",
    "        N = X.shape[0];\n",
    "        \n",
    "        D = np.zeros((N,self.n_clfs+1))        \n",
    "        D[:,0] = 1./N*np.ones((N,))  \n",
    "        \n",
    "        for t in np.arange(0,self.n_clfs):\n",
    "            print(t)\n",
    "            clf = DecisionTreeClassifier(max_leaf_nodes=n_nodes)\n",
    "            self.clfs.append(clf.fit(X,Y,sample_weight=D[:,t]))\n",
    "            h = self.clfs[t].predict(X)\n",
    " \n",
    "            e = np.sum(D[:,t]*np.sign(np.abs(Y-h)))\n",
    "            a = 0.5*np.log((1-e)/e)\n",
    "            self.coefs.append(a)\n",
    "            \n",
    "            D[:,t+1] = D[:,t]*np.exp(-a*Y*h)\n",
    "            D[:,t+1] /= np.sum(D[:,t+1])          \n",
    "             \n",
    "        return D\n",
    "    \n",
    "    pass\n",
    "\n",
    "    \n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Predict on the given dataset.\n",
    "\n",
    "        Inputs:\n",
    "            X: A (N, D) shaped numpy array containing the data points.\n",
    "            \n",
    "        Outputs:\n",
    "            A (N, ) shaped numpy array containing the (float) labels of the data points.\n",
    "            (Even though the labels are ints, we treat them as floats.)\n",
    "        '''\n",
    "        # Initialize predictions.\n",
    "        Y_pred = np.zeros(len(X))\n",
    "        \n",
    "        # Add predictions from each DT weak classifier.\n",
    "        for i, clf in enumerate(self.clfs):\n",
    "            Y_curr = self.coefs[i] * clf.predict(X)\n",
    "            Y_pred += Y_curr\n",
    "\n",
    "        # Return the sign of the predictions.\n",
    "        return np.sign(Y_pred)\n",
    "\n",
    "    def loss(self, X, Y):\n",
    "        '''\n",
    "        Calculate the classification loss.\n",
    "\n",
    "        Inputs:\n",
    "            X: A (N, D) shaped numpy array containing the data points.\n",
    "            Y: A (N, ) shaped numpy array containing the (float) labels of the data points.\n",
    "               (Even though the labels are ints, we treat them as floats.)\n",
    "            \n",
    "        Outputs:\n",
    "            The classification loss.\n",
    "        '''\n",
    "        # Calculate the points where the predictions and the ground truths don't match.\n",
    "        Y_pred = self.predict(X)\n",
    "        misclassified = np.where(Y_pred != Y)[0]\n",
    "\n",
    "        # Return the fraction of such points.\n",
    "        return float(len(misclassified)) / len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "Training loss: 0.386313\n",
      "Test loss: 0.388500\n"
     ]
    }
   ],
   "source": [
    "train = np.arange(0,16000)\n",
    "test = np.arange(16000,20000)\n",
    "\n",
    "model = AdaBoost(n_clfs=500)\n",
    "D = model.fit(X_train[train], y_train_pre[train])\n",
    "\n",
    "print('Training loss: %f' % model.loss(X_train[train], y_train_pre[train]))\n",
    "print('Test loss: %f' % model.loss(X_train[test], y_train_pre[test]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Fold 10 / 10\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 4s 230us/step - loss: 0.6152 - acc: 0.6983\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 1s 68us/step - loss: 0.3186 - acc: 0.8675\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 1s 69us/step - loss: 0.2084 - acc: 0.9186\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 1s 68us/step - loss: 0.1186 - acc: 0.9561\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 1s 67us/step - loss: 0.0638 - acc: 0.9766\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 1s 69us/step - loss: 0.0425 - acc: 0.9843\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 1s 68us/step - loss: 0.0359 - acc: 0.9875\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 1s 67us/step - loss: 0.0379 - acc: 0.9864\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 1s 83us/step - loss: 0.0350 - acc: 0.9881\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 1s 70us/step - loss: 0.0308 - acc: 0.9891\n",
      "Test score: 1.1143082467913628\n",
      "Test accuracy: 0.8225\n"
     ]
    }
   ],
   "source": [
    "## In the line below we have specified the loss function as 'mse' (Mean Squared Error) because in the above code we did not one-hot encode the labels.\n",
    "## In your implementation, since you are one-hot encoding the labels, you should use 'categorical_crossentropy' as your loss.\n",
    "## You will likely have the best results with RMS prop or Adam as your optimizer.  In the line below we use Adadelta\n",
    "\n",
    "n_folds = 10\n",
    "skf = StratifiedKFold(y_train_pre, n_folds=n_folds, shuffle=True)\n",
    "\n",
    "train = np.arange(0,16000)\n",
    "test = np.arange(16000,20000)\n",
    "\n",
    "#for i, (train, test) in enumerate(skf):\n",
    "print(\"Running Fold\", i+1, \"/\", n_folds)\n",
    "model = None # Clearing the NN.\n",
    "model = create_model()\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "fit = model.fit(X_train[train], y_train[train], batch_size=128, epochs=10, verbose=1)    \n",
    "score = model.evaluate(X_train[test], y_train[test], verbose=0)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Fold 1 / 5\n",
      "Test accuracy: 0.8482879280179955\n",
      "Running Fold 2 / 5\n",
      "Test accuracy: 0.8395401149712571\n",
      "Running Fold 3 / 5\n",
      "Test accuracy: 0.84725\n",
      "Running Fold 4 / 5\n",
      "Test accuracy: 0.8537134283570893\n",
      "Running Fold 5 / 5\n",
      "Test accuracy: 0.8394598649662416\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import zero_one_loss\n",
    "\n",
    "n_folds = 5\n",
    "skf = StratifiedKFold(y_train, n_folds=n_folds, shuffle=True)\n",
    "\n",
    "pred = np.zeros(10000)\n",
    "\n",
    "for i, (train, test) in enumerate(skf):\n",
    "    print(\"Running Fold\", i+1, \"/\", n_folds)\n",
    "    forest = RandomForestClassifier(n_estimators = 500, criterion='entropy',max_features='log2')\n",
    "\n",
    "    forest = forest.fit(X_train[train], y_train[train]) \n",
    "    acc = forest.score(X_train[test],y_train[test])\n",
    "    \n",
    "    pred += forest.predict(X_test)\n",
    "    \n",
    "    print('Test accuracy:', acc)\n",
    "\n",
    "y_labs = np.maximum(0.,np.sign(pred-2.5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"labels.txt\",\"w\") \n",
    "file.write(\"Id,Prediction\\n\")\n",
    "for i,x in enumerate(y_labs):\n",
    "    file.write(\"%i,%i\\n\" % (i+1,x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_labs = np.maximum(0.,np.sign(pred-2.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 0., ..., 0., 1., 0.])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_labs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
